{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saved States Analysis (Simple)\n",
    "\n",
    "Visualizations for analyzing LLM intermediate activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load session\n",
    "session_path = \"saved_states/20251203_000329_france_no_attn_4\" # update this path as needed\n",
    "items = np.load(f\"{session_path}/tensors.npz\")\n",
    "with open(f\"{session_path}/config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Extract data\n",
    "print(\"The npz has keys:\", items.files)\n",
    "hidden_states = items['hidden_states'].squeeze(1) \n",
    "layer_names = items['layer_names']\n",
    "num_layers, seq_len, dim = hidden_states.shape\n",
    "\n",
    "print(f\"Text: {config['text']}\")\n",
    "print(f\"Model: {config['model_name']}\")\n",
    "print(f\"Hidden states shape: {hidden_states.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    dot = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "    return dot / (norm_a * norm_b)\n",
    "\n",
    "token_labels = [f\"T{i}\" for i in range(seq_len)]\n",
    "num_blocks = (num_layers - 1) // 2\n",
    "block_labels = [f\"L{i}\" for i in range(num_blocks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cosine Similarity Between Consecutive Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between consecutive layers for each token\n",
    "cosine_sim_consecutive = np.zeros((num_layers - 1, seq_len))\n",
    "\n",
    "for layer_idx in range(num_layers - 1):\n",
    "    for token_idx in range(seq_len):\n",
    "        cosine_sim_consecutive[layer_idx, token_idx] = cosine_similarity(\n",
    "            hidden_states[layer_idx, token_idx],\n",
    "            hidden_states[layer_idx + 1, token_idx]\n",
    "        )\n",
    "\n",
    "# Create labels for layer transitions\n",
    "transition_labels = [f\"{layer_names[i]} â†’ {layer_names[i+1]}\" for i in range(num_layers - 1)]\n",
    "\n",
    "# Plot cosine similarity heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(\n",
    "    cosine_sim_consecutive,\n",
    "    xticklabels=token_labels,\n",
    "    yticklabels=transition_labels,\n",
    "    cmap='RdYlGn',\n",
    "    vmin=0, vmax=1,\n",
    "    cbar_kws={'label': 'Cosine Similarity'}\n",
    ")\n",
    "plt.title(f'Cosine Similarity Between Consecutive Layers\\n{config[\"model_name\"]}')\n",
    "plt.xlabel('Token Position')\n",
    "plt.ylabel('Layer Transition')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Statistics per Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics per layer\n",
    "stats = {\n",
    "    'mean': hidden_states.mean(axis=(1, 2)),\n",
    "    'std': hidden_states.std(axis=(1, 2)),\n",
    "    'min': hidden_states.min(axis=(1, 2)),\n",
    "    'max': hidden_states.max(axis=(1, 2)),\n",
    "    'abs_mean': np.abs(hidden_states).mean(axis=(1, 2)),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Mean activation\n",
    "axes[0, 0].plot(stats['mean'], marker='o', markersize=4, label='Mean')\n",
    "axes[0, 0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].set_xticks(range(len(layer_names)))\n",
    "axes[0, 0].set_xticklabels(layer_names, rotation=90)\n",
    "axes[0, 0].set_ylabel('Mean Activation')\n",
    "axes[0, 0].set_title('Mean Activation per Layer')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Std activation\n",
    "axes[0, 1].plot(stats['std'], marker='s', markersize=4, color='orange')\n",
    "axes[0, 1].set_xticks(range(len(layer_names)))\n",
    "axes[0, 1].set_xticklabels(layer_names, rotation=90)\n",
    "axes[0, 1].set_ylabel('Std Deviation')\n",
    "axes[0, 1].set_title('Activation Std Dev per Layer')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Min/Max range\n",
    "axes[1, 0].fill_between(range(len(layer_names)), stats['min'], stats['max'], alpha=0.3)\n",
    "axes[1, 0].plot(stats['min'], marker='v', markersize=3, label='Min')\n",
    "axes[1, 0].plot(stats['max'], marker='^', markersize=3, label='Max')\n",
    "axes[1, 0].set_xticks(range(len(layer_names)))\n",
    "axes[1, 0].set_xticklabels(layer_names, rotation=90)\n",
    "axes[1, 0].set_ylabel('Activation Value')\n",
    "axes[1, 0].set_title('Activation Range (Min/Max) per Layer')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Absolute mean\n",
    "axes[1, 1].plot(stats['abs_mean'], marker='d', markersize=4, color='green')\n",
    "axes[1, 1].set_xticks(range(len(layer_names)))\n",
    "axes[1, 1].set_xticklabels(layer_names, rotation=90)\n",
    "axes[1, 1].set_ylabel('Mean |Activation|')\n",
    "axes[1, 1].set_title('Average Absolute Activation per Layer')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Residual Stream vs Attention/MLP Output Magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention and MLP outputs by taking differences\n",
    "# For block i:\n",
    "#   Attention output = hidden_states[2*i + 1] - hidden_states[2*i]\n",
    "#   MLP output = hidden_states[2*i + 2] - hidden_states[2*i + 1]\n",
    "\n",
    "attention_outputs = []\n",
    "mlp_outputs = []\n",
    "residual_at_block_input = []\n",
    "\n",
    "for i in range(num_blocks):\n",
    "    prev_idx = 2 * i\n",
    "    post_attn_idx = 2 * i + 1\n",
    "    block_out_idx = 2 * i + 2\n",
    "    \n",
    "    attention_outputs.append(hidden_states[post_attn_idx] - hidden_states[prev_idx])\n",
    "    mlp_outputs.append(hidden_states[block_out_idx] - hidden_states[post_attn_idx])\n",
    "    residual_at_block_input.append(hidden_states[prev_idx])\n",
    "\n",
    "attention_outputs = np.array(attention_outputs)\n",
    "mlp_outputs = np.array(mlp_outputs)\n",
    "residual_at_block_input = np.array(residual_at_block_input)\n",
    "\n",
    "# Compute L2 norms\n",
    "residual_l2 = np.linalg.norm(residual_at_block_input, axis=-1)\n",
    "attn_out_l2 = np.linalg.norm(attention_outputs, axis=-1)\n",
    "mlp_out_l2 = np.linalg.norm(mlp_outputs, axis=-1)\n",
    "\n",
    "# Average over tokens\n",
    "avg_residual_l2 = residual_l2.mean(axis=1)\n",
    "avg_attn_l2 = attn_out_l2.mean(axis=1)\n",
    "avg_mlp_l2 = mlp_out_l2.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Line plot comparison\n",
    "ax = axes[0, 0]\n",
    "ax.plot(avg_residual_l2, marker='o', markersize=5, label='Residual Stream', linewidth=2)\n",
    "ax.plot(avg_attn_l2, marker='s', markersize=5, label='Attention Output', linewidth=2)\n",
    "ax.plot(avg_mlp_l2, marker='^', markersize=5, label='MLP Output', linewidth=2)\n",
    "ax.set_xticks(range(num_blocks))\n",
    "ax.set_xticklabels(block_labels)\n",
    "ax.set_xlabel('Block')\n",
    "ax.set_ylabel('Average L2 Norm')\n",
    "ax.set_title('L2 Norm Comparison: Residual vs Attention vs MLP')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Ratio: Attention/Residual and MLP/Residual\n",
    "ax = axes[0, 1]\n",
    "attn_ratio = avg_attn_l2 / avg_residual_l2\n",
    "mlp_ratio = avg_mlp_l2 / avg_residual_l2\n",
    "ax.plot(attn_ratio, marker='s', markersize=5, label='Attention / Residual', linewidth=2)\n",
    "ax.plot(mlp_ratio, marker='^', markersize=5, label='MLP / Residual', linewidth=2)\n",
    "ax.set_xticks(range(num_blocks))\n",
    "ax.set_xticklabels(block_labels)\n",
    "ax.set_xlabel('Block')\n",
    "ax.set_ylabel('Ratio')\n",
    "ax.set_title('Output Magnitude Relative to Residual Stream')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Stacked bar\n",
    "ax = axes[1, 0]\n",
    "x = np.arange(num_blocks)\n",
    "width = 0.6\n",
    "ax.bar(x, avg_attn_l2, width, label='Attention', alpha=0.8)\n",
    "ax.bar(x, avg_mlp_l2, width, bottom=avg_attn_l2, label='MLP', alpha=0.8)\n",
    "ax.plot(x, avg_residual_l2, 'ko-', markersize=6, label='Residual (reference)', linewidth=2)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(block_labels)\n",
    "ax.set_xlabel('Block')\n",
    "ax.set_ylabel('L2 Norm')\n",
    "ax.set_title('Attention + MLP vs Residual Stream')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Percentage contribution\n",
    "ax = axes[1, 1]\n",
    "total_contribution = avg_attn_l2 + avg_mlp_l2\n",
    "attn_pct = (avg_attn_l2 / total_contribution) * 100\n",
    "mlp_pct = (avg_mlp_l2 / total_contribution) * 100\n",
    "ax.bar(x, attn_pct, width, label='Attention %', alpha=0.8)\n",
    "ax.bar(x, mlp_pct, width, bottom=attn_pct, label='MLP %', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(block_labels)\n",
    "ax.set_xlabel('Block')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_title('Relative Contribution: Attention vs MLP')\n",
    "ax.legend()\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cosine Similarity: Residual vs Attention/MLP Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between residual stream and attention/MLP outputs\n",
    "cosine_residual_attn = np.zeros((num_blocks, seq_len))\n",
    "cosine_residual_mlp = np.zeros((num_blocks, seq_len))\n",
    "\n",
    "for block_idx in range(num_blocks):\n",
    "    for token_idx in range(seq_len):\n",
    "        residual_vec = residual_at_block_input[block_idx, token_idx]\n",
    "        attn_vec = attention_outputs[block_idx, token_idx]\n",
    "        mlp_vec = mlp_outputs[block_idx, token_idx]\n",
    "        \n",
    "        cosine_residual_attn[block_idx, token_idx] = cosine_similarity(residual_vec, attn_vec)\n",
    "        cosine_residual_mlp[block_idx, token_idx] = cosine_similarity(residual_vec, mlp_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmaps of cosine similarity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "im0 = axes[0].imshow(cosine_residual_attn, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[0].set_xlabel('Token Position')\n",
    "axes[0].set_ylabel('Block')\n",
    "axes[0].set_yticks(range(num_blocks))\n",
    "axes[0].set_yticklabels(block_labels)\n",
    "axes[0].set_title('Cosine Similarity: Residual vs Attention Output')\n",
    "plt.colorbar(im0, ax=axes[0], label='Cosine Similarity')\n",
    "\n",
    "im1 = axes[1].imshow(cosine_residual_mlp, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[1].set_xlabel('Token Position')\n",
    "axes[1].set_ylabel('Block')\n",
    "axes[1].set_yticks(range(num_blocks))\n",
    "axes[1].set_yticklabels(block_labels)\n",
    "axes[1].set_title('Cosine Similarity: Residual vs MLP Output')\n",
    "plt.colorbar(im1, ax=axes[1], label='Cosine Similarity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logit-lens-llama-advanced (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
